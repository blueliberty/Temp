{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "+ 1 前言\n",
    "+ 2 数据\n",
    "+ 3 标签的类型\n",
    "+ 4 评价指标\n",
    "+ 5 库\n",
    "+ 6 机器学习框架\n",
    "    + 6.1 切分训练集、验证集\n",
    "    + 6.2 特征工程\n",
    "        + 6.2.1 数值特征\n",
    "        + 6.2.2 类别特征\n",
    "        + 6.2.3 文本特征\n",
    "    + 6.3 特征融合\n",
    "    + 6.4 模型建立\n",
    "        + 6.4.1 非线性模型\n",
    "        + 6.4.2 线性模型\n",
    "    + 6.5 特征降维\n",
    "    + 6.6 特征选择\n",
    "    + 6.7 模型选择 + 参数调优\n",
    "+ 7 结语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 前言\n",
    "机器学习流程通常分为两个阶段：\n",
    "\n",
    "+ Step1. 数据清洗，数据格式调整\n",
    "+ Step2. 特征工程，模型建立，效果评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 数据\n",
    "在机器学习建模之前，数据必须被转换为表格形式。这一步骤是整个过程中最为耗时、困难的步骤，具体方法可以用下图展示：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_1.png)\n",
    "\n",
    "然后将机器学习模型应用于表格型数据上。在机器学习、数据挖掘领域，通常用表格型数据来进行数据展示。在数据表中，行代表样本，列代表特征。标签可以是一列或多列，取决于问题的类型。用X表示特征，用y表示标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 标签的类型\n",
    "标签的类型决定了问题的类型，例如：\n",
    "+ 二分类问题（这种问题在工业界最为常见，比如广告点击率预估、推荐系统购买行为预测），此时y只有一维，取值只有两个(比如0-1)，每个样本有唯一的标签。比如预测广告是否会被用户点击；用户是否会购买某种商品\n",
    "+ 多分类问题（比如微博用户情感分析、用户对理财产品偏好性分析），通常此时y有多维，每维代表一个类标签，取值只有两个（比如0-1），每个样本有唯一的标签；当然，y也可以只有一维，取值有多个，每个值代表一个类标签。比如通过微博分析出用户情感属于喜怒哀乐等哪类；将理财产品的用户群体分为偏好型/温和型/厌恶型\n",
    "+ 多标签问题（比如音乐的标签划分），y有多维，跟多分类的区别在于，样本可以同时属于多个标签。作为一枚钢琴爱好者，这里以钢琴作品举例，假设标签集合为{独奏，协奏，浪漫主义，印象主义}，最爱之一的德彪西「月光」无疑属于{独奏，印象主义}，朗总成名作柴一则可归为{协奏，浪漫主义}，云迪家喻户晓的肖邦夜曲是{独奏，浪漫主义}，而中国特色的「保卫黄河」可归为{协奏}\n",
    "+ 单回归问题（比如股价预测），y只有一维，取值为连续值。比如预测阿里明天的股价\n",
    "+ 多回归问题（比如天气预测），y有多维，取值连续。比如预测明天的气温、空气湿度、降雨量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 评价指标\n",
    "对于任何机器学习问题，我们都必须知道如何评价模型的预测结果，即评价指标。例如：对于类别不平衡的二分类问题，我们通常使用AUC作为评价指标；对于多分类或多标签问题，我们通常选择类别交叉熵（categorical cross-entropy）或多类别log损失（multiclass log loss）作为评价指标；对于回归问题，则是均方误差（mean squared error）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 库\n",
    "+ [pandas](http://pandas.pydata.org/): To see and do operations on data\n",
    "+ [scikit-learn](http://scikit-learn.org/stable/): For all kinds of machine learning models\n",
    "+ [xgboost](https://github.com/dmlc/xgboost): The best gradient boosting library\n",
    "+ [keras](http://keras.io/): For neural networks\n",
    "+ [matplotlib](http://matplotlib.org/): For plotting data\n",
    "+ [tpdm](https://pypi.python.org/pypi/tqdm): To monitor progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6 机器学习框架\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_2.png)\n",
    "\n",
    "在上图所展示的机器学习框架中，粉色线条代表了最通常的路线图。当我们提取了数据并将其转换为表格形式后，就可以着手构建机器学习模型了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 切分训练集、验证集\n",
    "\n",
    "首先通过观察标签来确定问题的类型。然后将数据切分为训练集和验证集，如下图所示：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "对数据集的切分必须依据标签的形式。对于分类问题，需要使用分层切分。在Python中的代码如下：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "对于回归问题，可以使用简单的K折切分。也有一些复杂的方法，可以保证训练集和测试集的标签分布相同。\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_5.png)\n",
    "\n",
    "在这个例子中，验证集的样本占整个数据集的10%，这一比例可以另行选取。\n",
    "\n",
    "对数据集进行切分后，任何在训练集上进行的操作，都必须被应用到验证集上。验证集和训练集不能存在交叉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "对于回归问题，可以使用简单的K折切分。也有一些复杂的方法，可以保证训练集和测试集的标签分布相同。\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_5.png)\n",
    "\n",
    "在这个例子中，验证集的样本占整个数据集的10%，这一比例可以另行选取。\n",
    "\n",
    "对数据集进行切分后，任何在训练集上进行的操作，都必须被应用到验证集上。验证集和训练集不能存在交叉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6.2 特征工程\n",
    "\n",
    "下一步是确定特征的数据类型：数值特征、类别特征、文本特征。以[Titanic数据集](https://www.kaggle.com/c/titanic/data)为例：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_6.png)\n",
    "\n",
    "其中 ``survival`` 是标签，``pclass``, ``sex``, ``embarked`` 是类别特征，``age``, ``sibsp``, ``parch`` 等特征是数值特征，``name`` 是文本特征。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 数值特征\n",
    "\n",
    "将数值特征先分离出来，这些特征不需要太多的数据预处理，只需要做归一化``(normalization)``和标准化``(standardization)``即可，分别对应 ``scikit-learn`` 中的 ``Normalizer`` 和 ``StandardScaler``。不过对于稀疏特征，在做标准化的时候要注意，选择不去均值(parameter: with_mean=False)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6.2.2 类别特征\n",
    "\n",
    "对于类别特征，有两种方法可以处理：\n",
    "\n",
    "+ 将类别特征转换为数值标签\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_7.png)\n",
    "\n",
    "+ 将标签转换为0-1特征（独热编码）\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_8.png)\n",
    "\n",
    "先用 ``LabelEncoder`` 将类别特征转换为数值标签，再用 ``OneHotEncoder``进行独热编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 文本特征\n",
    "\n",
    "对于文本特征，我们可以先将所有文本拼接到一起，再用算法将文本特征转换为数值特征。\n",
    "\n",
    "文本的拼接可以用以下代码实现：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_9.png)\n",
    "\n",
    "然后对其应用 ``CountVectorizer`` 或 ``TfidfVectorizer``:\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_10.png)\n",
    "或\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_11.png)\n",
    "\n",
    "``TfidfVectorizer`` 的效果一般优于 ``CountVectorizer``。下列参数设置可以用于几乎所有文本特征的处理：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_12.png)\n",
    "\n",
    "如果上述对文本特征的处理仅仅用于训练集，确保将模型保存到硬盘中，以便随后将其用于验证集。\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6.3 特征融合\n",
    "\n",
    "接下来要进入 ``stacker`` 模块，将经过上述步骤处理过的特征组合起来。\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_14.png)\n",
    "\n",
    "根据特征是否为稀疏矩阵，可以分别使用 ``numpy.hstack`` 或 ``sparse.hstack`` ：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_15.png)\n",
    "\n",
    "如果之前做过PCA或特征选择，也可以使用 ``FeatureUnion`` 模块：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6.4 模型建立\n",
    "\n",
    "### 6.4.1 非线性模型\n",
    "\n",
    "将特征组合到一起后，可以开始建立机器学习模型。我们优先使用 ``ensemble tree based models`` ：\n",
    "+ RandomForestClassifier\n",
    "+ RandomForestRegressor\n",
    "+ ExtraTreesClassifier\n",
    "+ ExtraTreesRegressor\n",
    "+ XGBClassifier\n",
    "+ XGBRegressor\n",
    "\n",
    "### 6.4.2 线性模型\n",
    "\n",
    "如果要使用线性模型，则必须利用 ``scikit-learn`` 中的 ``Normalizer`` 和 ``StandardScaler`` 对特征进行规范化处理。\n",
    "\n",
    "这些规范化处理方法仅仅用于非稀疏矩阵 ``(dense features)``，对稀疏矩阵使用的效果很差。我们可以用不设置 ``mean`` 参数的 ``StandardScaler`` 来处理稀疏矩阵 ``(parameter: with_mean=False)``。\n",
    "\n",
    "如果上述步骤能够得到一个效果良好的模型，就可以进行参数调优。否则，需要进行以下步骤来对模型进行改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6.5 特征降维\n",
    "\n",
    "对模型的改进包括分解方法 ``(decomposition methods)``：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_17.png)\n",
    "\n",
    "为了简化问题，此处略过 ``LDA``和 ``QDA`` 转换。对于高维数据，通常使用 ``PCA`` 进行分解。对于图像 ``images `` 处理问题，初始选择``10-15``个主成分，并不断提高主成分个数，直至效果不再显著提升。对其他类型的数据，初始选择``50-60``个主成分。对于数值特征，只要能够处理得了，就尽量不要使用 ``PCA`` 方法。\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于文本特征，在将文本转换为稀疏矩阵后，使用 ``scikit-learn`` 中的 ``TruncatedSVD`` 进行奇异值分解 ``(Singular Value Decomposition)``。\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_decomp.png)\n",
    "\n",
    "对``TfidfVectorizer`` 或 ``CountVectorizer`` 处理后得到稀疏矩阵进行 ``SVD``，主成分一般选择 ``120-200``。过高的主成分个数非但不能显著提高模型效果，还会引起计算资源的浪费。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 特征选择\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有多种方法可以进行特征选择。贪心特征选择（前向或后向）是最常用的特征选择方法之一。在 https://github.com/abhishekkrthakur/greedyFeatureSelection 可以找到一个贪心特征选择的实现（用 ``AUC`` 作为评价指标）。值得注意的是，该实现并不完美，需要根据要求进行变化或修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从模型中选出最优的特征，可以更快速的进行特征选择。我们可以观察 ``logistics`` 回归的系数，也通过随机森林模型选出最优的特征，并将选出的特征用于之后的其他模型。\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_20.png)\n",
    "\n",
    "注意保持较小的 ``n_estimators`` 参数值，并进行尽量少的参数调优，以防止过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以通过 ``Gradient Boosting Machines`` 模型进行特征选择。由于 ``xgboost`` 模型的速度更快、扩展性更强，也可以用 ``xgboost`` 模型来替代 ``scikit-learn`` 中的 ``GBM`` 模型。\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_21.png)\n",
    "\n",
    "我们也可以用 ``RandomForestClassifier`` 、 ``RandomForestRegressor`` 、 ``xgboost`` 模型来对稀疏数据集进行特征选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一种基于卡方 ``(chi-2 based)`` 的方法，可以对正定稀疏数据集 ``(positive sparse datasets)`` 进行特征选择。\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_22.png)\n",
    "\n",
    "在这里我们联合使用 ``chi2`` 和 ``SelectKBest`` 从数据中选出20个特征。为了提升模型的效果，这里也需要进行参数调优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步骤涉及的转换器都要保存到硬盘中，方便后续对验证集进行同样的转换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 模型选择 + 参数调优\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通常在下列模型中进行选择：\n",
    "+ Classification:\n",
    "    + Random Forest\n",
    "    + GBM\n",
    "    + Logistic Regression\n",
    "    + Naive Bayes\n",
    "    + Support Vector Machines\n",
    "    + k-Nearest Neighbors\n",
    "    \n",
    "    \n",
    "+ Regression\n",
    "    + Random Forest\n",
    "    + GBM\n",
    "    + Linear Regression\n",
    "    + Ridge\n",
    "    + Lasso\n",
    "    + SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调参建议：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_24.png)\n",
    "\n",
    "RS\\* = 无法确定合适的参数值，使用 ``Random Search`` 来对这些参数进行调优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再一次强调，记得保存转换器：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_25.png)\n",
    "\n",
    "并将它们用于验证集：\n",
    "\n",
    "![](https://raw.githubusercontent.com/blueliberty/Kaggle/master/Tutorial/abhishek_26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 结语\n",
    "上述规则和框架在大多数数据集中都表现良好，但是也可能无法应付非常复杂的任务。没有什么事情是完美的，我们需要不断改进已经学到的东西。就如同机器学习算法一样。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
